{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec-tutorial.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1DM_EVu2BnnCgo_mXA1EW3Um-ZwLUhexN","authorship_tag":"ABX9TyOwPRwYRzyRPTbY0Tpjcsyh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vcZ9EwXbUWIW"},"source":["# ライブラリのインストール (必要な方だけ)"]},{"cell_type":"markdown","metadata":{"id":"hafsKYYDUdnQ"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"1nHobXgqUzr9"},"source":["# ソースコード\n","## ディープラーニングモジュールのimport"]},{"cell_type":"code","metadata":{"id":"r0RZdSmDF-4A","executionInfo":{"status":"ok","timestamp":1603010273823,"user_tz":-540,"elapsed":919,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}}},"source":["# pytroch(ディープラーニングに必要なモジュール)　をインポートして利用できるようにする。\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":122,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzSE5PqOVchr"},"source":["## 学習に必要なデータの作成"]},{"cell_type":"code","metadata":{"id":"qLPjxL_SNUzy","executionInfo":{"status":"ok","timestamp":1603010273825,"user_tz":-540,"elapsed":912,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}}},"source":["CONTEXT_SIZE = 2 # 文脈の情報\n","EMBEDDING_SIZE = 10 # 各語彙に割り当てるベクトルの次元数"],"execution_count":123,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w32LRra0Xj29"},"source":["`CONTEXT_SIZE`はどれだけの周辺単語を考慮するかを決定する変数。例えば4とすれば、ターゲットにした語彙の周辺４語彙を考慮して学習データが作成される  \n","`EMBEDDING_SIZE`は各語彙に割り当てるベクトルの次元数"]},{"cell_type":"code","metadata":{"id":"7OXHAK-EXOVb","executionInfo":{"status":"ok","timestamp":1603010273826,"user_tz":-540,"elapsed":905,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}}},"source":["sentences = \"\"\"「 クリスティーヌ 。 お前 に は 隣国 の 貴族 に 嫁い で もらう 」 「 … … え ？ 」 私 は 驚い て 目 を 見開い た 。 目 の 前 に は 立派 な 口髭 を 生やし た 壮年 の 男 が 、 私 を 見下ろし て いる 。 その 周り に は 、 世 に も 美しい 顔立ち の し た 金髪 碧眼 の 青年 や 、 眼鏡 を 掛け た クール な 美 男子 や 、 その 顔 に あどけな さ を 残す 可愛らしい 美少年 が 、 総じて 私 を 睨ん で い た 。 そして もう 一人 、 素朴 な 可愛らし さ が ある 少女 が 、 困っ た 顔 で 私 を 見 て いる 。 私 は クリスティーヌ ・ ブラン シャーネ 。 王国 有数 の ブラン シャーネ 公爵 の 一人娘 で 、 小さい 頃 から 蝶 よ 花 よ と 育て られ 、 それ は それ は 傲慢 な 娘 に 育っ た 。 生まれ た 時 から 、 望ん だ もの は 全部 与え られる 。 そして それ を 当然 だ と 思っ て い た 。\"\"\""],"execution_count":124,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AB7kZwkiXWiO"},"source":["上記の半角スペースで区切られた文章を整形していき、学習データを作成する。  \n","実際の収集してきたデータはスペースで区切られていないが、これには形態素解析ツールを利用するとこのような形で分割できる。"]},{"cell_type":"code","metadata":{"id":"qME5in5YG60L","executionInfo":{"status":"ok","timestamp":1603010273828,"user_tz":-540,"elapsed":902,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}}},"source":["vocab_sequence = sentences.split() # 半角スペースで文字を区切る\n","vocab_set = set(vocab_sequence) # set コンストラクタにリストを渡して要素の重複をなくした\n","word2id = {word: id for id, word in enumerate(vocab_set)}# word → idを得るための辞書の作成\n","id2word = {id: word for word, id in word2id.items()} # id →　wordを得るための辞書の作成\n","train_data = [\n","  (\n","    [word2id[vocab_sequence[target - CONTEXT_SIZE + j]] for j in range(CONTEXT_SIZE * 2 + 1) if target - CONTEXT_SIZE + j != target], # ターゲットに対するコンテキストデータをまとめます。\n","    word2id[vocab_sequence[target]] # ラベルデータ(正解)\n","  )\n","  for target in range(CONTEXT_SIZE, len(vocab_sequence) - CONTEXT_SIZE - 1)\n","] # 学習データの作成"],"execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ROcYB4yb-bI","executionInfo":{"status":"ok","timestamp":1603010273829,"user_tz":-540,"elapsed":898,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}},"outputId":"71e1a74e-f44f-4183-d6c9-47d2252383b0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_data[:3] # 0番目から2番目までの学習データを確認"],"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[([10, 41, 4, 16], 0), ([41, 0, 16, 95], 4), ([0, 4, 95, 59], 16)]"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"code","metadata":{"id":"U7ykAhu0s0J1","executionInfo":{"status":"ok","timestamp":1603010273832,"user_tz":-540,"elapsed":893,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}},"outputId":"b83293b3-d57e-4168-862b-f5cf68fe5d5b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_data[-1]"],"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([53, 40, 21, 6], 32)"]},"metadata":{"tags":[]},"execution_count":127}]},{"cell_type":"markdown","metadata":{"id":"0ctUuCJfcXl9"},"source":["これで学習データの準備が完了"]},{"cell_type":"markdown","metadata":{"id":"_Tmb_V26ckD5"},"source":["## 学習モデルを定義\n","CBOWとskip-gramがあると説明しましたが、ここではより単純なモデルであるCBOWのモデルを定義します。 \n","最低限必要な実装は`forward`のみです。データをどのように処理していくのかをここに記述します。\n","\n","\n","forwardに入力する行列x（テンソル）の構造の変化\n","```\n","(N × CONTEXT_SIZE)   \n","→embed→ (CONTEXT_SIZE × embedding_dim)  \n","→view((1,-1))→ ((CONTEXT_SIZE * embedding_dim))  \n","→linearA(x)→ (hidden_dim)\n","→F.relu(x)→ (hidden_dim)\n","→linearB(x)→　(vocab_size)\n","→F.log_softmax(x, dim=1)→　(vocab_size)\n","```"]},{"cell_type":"code","metadata":{"id":"5ekNzTdROt-c","executionInfo":{"status":"ok","timestamp":1603010273834,"user_tz":-540,"elapsed":887,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}}},"source":["class CBOW(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","    super(CBOW, self).__init__()\n","    self.embed = nn.Embedding(vocab_size, embedding_dim) # embedding により語彙IDをそれぞれに対応するベクトルに変換する\n","    self.linearA = nn.Linear(CONTEXT_SIZE * 2 * embedding_dim, hidden_dim) # 全結合層。CONTEXT_SIZE個の学習データをまとめて入力したいので、入力次元数をCONTEXT_SIZE * 2 * embedding_dimのようにしている。\n","    self.linearB = nn.Linear(hidden_dim, vocab_size)\n","    pass\n","\n","  def forward(self, x): # 順伝播を定義します。\n","    # x は N × CONTEXT_SIZEのtensor.　ここで、Nはバッチデータサイズ\n","    x = self.embed(x).view((1, -1))\n","    x = self.linearA(x)\n","    x = F.relu(x)\n","    x = self.linearB(x)\n","    return F.log_softmax(x, dim=1)\n"],"execution_count":128,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1mhK8R7l29o"},"source":["## トレーニングの実行"]},{"cell_type":"code","metadata":{"id":"R7rFRNquUhUc","executionInfo":{"status":"ok","timestamp":1603010282514,"user_tz":-540,"elapsed":9552,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}},"outputId":"199403c3-9293-43f5-b38c-9cd6003cc9da","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["vocab_size = len(word2id) # 語彙数\n","model = CBOW(vocab_size, EMBEDDING_SIZE, 128) # クラスからインスタンスを作成\n","nlloss_function = nn.NLLLoss() #　使用する損失関数の宣言\n","optimizer = optim.SGD(model.parameters(), lr=0.01) # modelのパラメータをSGD最適化モジュールに設定\n","epoch_num = 100 # 学習回数\n","\n","for epoch in range(epoch_num):\n","  loss_sum = 0\n","  for contexts, target in train_data:\n","    model.zero_grad()\n","    contexts = torch.tensor(contexts, dtype=torch.long)\n","    labels = torch.tensor([target], dtype=torch.long)\n","\n","    out = model(contexts) # モデルにデータを入力\n","\n","    loss = nlloss_function(out, labels) # 正解ラベルとの誤差を計算\n","    loss.backward()\n","    optimizer.step() # パラメータの修正を行う\n","    loss_sum += loss.item() # 損失を加算\n","  if (epoch+1) % 10 == 0: # 10回ごとに損失の平均値を出力\n","    print('loss', loss_sum / epoch_num)\n"],"execution_count":129,"outputs":[{"output_type":"stream","text":["loss 3.496219913363457\n","loss 0.44183608192950485\n","loss 0.15740457333624364\n","loss 0.1010095592495054\n","loss 0.07747589786187746\n","loss 0.0642921561805997\n","loss 0.05572149791172706\n","loss 0.04965983530622907\n","loss 0.04513972062733956\n","loss 0.04160438053542748\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gplPYeMlxDsI"},"source":["損失が徐々に下がっていっている。つまり、モデルが学習データによってトレーニングされていっている。"]},{"cell_type":"markdown","metadata":{"id":"Huim-aiPxL6M"},"source":["## 学習したモデルの挙動を確認する\n","学習データの一部「私を見下ろしている」を利用して、ちゃんと学習したものに答えられるのかどうかを確認してみる。"]},{"cell_type":"code","metadata":{"id":"ZV-cRnrLwIb1","executionInfo":{"status":"ok","timestamp":1603010282515,"user_tz":-540,"elapsed":9514,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}},"outputId":"adc1c6e3-d914-47f9-cdc6-61a00b068262","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["parsed = \"私 を 見下ろし て いる\".split() # 学習データ中のデータ\n","test_data = [word2id[word] for word in parsed]\n","test_data = test_data[:CONTEXT_SIZE] + test_data[CONTEXT_SIZE+1:CONTEXT_SIZE + 1 + CONTEXT_SIZE]\n","print('入力するデータ:', [id2word[word_id] for word_id in test_data], 'をidに変換したもの')"],"execution_count":130,"outputs":[{"output_type":"stream","text":["入力するデータ: ['私', 'を', 'て', 'いる'] をidに変換したもの\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ykT8RLlj0TTU"},"source":["入力するデータは上記の通りになる。"]},{"cell_type":"code","metadata":{"id":"wxZdXt1oyeeZ","executionInfo":{"status":"ok","timestamp":1603010282516,"user_tz":-540,"elapsed":9453,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}}},"source":["test_data = torch.tensor(test_data)\n","p = model(test_data) # テストデータを入れた場合の語彙を計算する。"],"execution_count":131,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mLCt147jzvI","executionInfo":{"status":"ok","timestamp":1603010282517,"user_tz":-540,"elapsed":9445,"user":{"displayName":"古川拓実","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk3wbURQh0QbHI8qGATInK8ebSudvClxg16whZ=s64","userId":"17797839226158286464"}},"outputId":"f63ea24a-9b2e-4883-f08b-12828f1269a1","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["for idx, i in enumerate(torch.topk(p, 5, largest=True).indices[0]): # torch.topkで降順に５つの値を取得し、そのインデックスをforで回している。\n","  print(idx + 1, id2word[i.item()])"],"execution_count":132,"outputs":[{"output_type":"stream","text":["1 見\n","2 見下ろし\n","3 驚い\n","4 睨ん\n","5 生やし\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K65bmWM70wou"},"source":["本当の正解は「見下ろし」だが、近い意味を持つ「見」が一番上に来ていることがわかる。  \n","つまり、「見下ろし」と「見」に割り当てられたベクトルがトレーニングによって近いベクトルになったというわけである。"]},{"cell_type":"markdown","metadata":{"id":"Nj8HRfoN2OMH"},"source":["なお、この程度の学習データ量で、このような好ましい結果が得られることの方が稀である  \n","実際の利用に耐えうるようなベクトル表現を学習させようと思うなら、もっとたくさんの学習データが必要であることを留意して欲しい。\n"]}]}